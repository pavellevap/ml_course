{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.train = True\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (a black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is performed by applying a `forward` function: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    Moreover, it should be able to differentiate it if is a part of chain (by chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    \n",
    "    def _flatten(a):\n",
    "        if type(a) == int or type(a) == float or type(a) == bool:\n",
    "            return [a]\n",
    "        else:\n",
    "            return np.concatenate([self._flatten(b) for b in a])\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        \n",
    "        Make sure to both store the data in `output` field and return it. \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        self.gradInput = gradOutput \n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def updateParameters(self, delta):\n",
    "        \"\"\"\n",
    "        Adds delta to parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def setParameters(self, parameters):\n",
    "        \"\"\"\n",
    "        Sets parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return np.array([])\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return np.array([])\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define** forward and backward pass procedures for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__() # this allows for calling parent ('Module') methods\n",
    "                                           # from a child ('Sequential') class\n",
    "        self.modules = []\n",
    "        self.m_inputs = None\n",
    "        self.m_params_count = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "        self.m_params_count.append(len(module.getParameters()))\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        m_inputs = [] \n",
    "        t = input\n",
    "        \n",
    "        for i, mod in enumerate(self.modules):\n",
    "            m_inputs.append(t)\n",
    "            t = mod.forward(t)\n",
    "            \n",
    "        self.output = t\n",
    "        self.m_inputs = m_inputs\n",
    "                \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        yinp = self.m_inputs[::-1]\n",
    "        t = gradOutput\n",
    "        \n",
    "        for i, mod in enumerate(self.modules[::-1]):\n",
    "            t = mod.backward(yinp[i], t)\n",
    "                \n",
    "        self.gradInput = t\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "            \n",
    "    def setParameters(self, parameters):\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            left = np.sum(self.m_params_count[:i])\n",
    "            right = np.sum(self.m_params_count[:i + 1])\n",
    "            mod.setParameters(parameters[left:right])\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = []\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            params.append(mod.getParameters())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        params = []\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            params.append(mod.getGradParameters())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def updateParameters(self, delta):\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            left = np.sum(self.m_params_count[:i])\n",
    "            right = np.sum(self.m_params_count[:i + 1])\n",
    "            mod.updateParameters(delta[left:right])\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return \"Sequential\"\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Merge(Module):\n",
    "    def __init__ (self):\n",
    "        super(Merge, self).__init__() # this allows for calling parent ('Module') methods\n",
    "                                      # from a child ('Sequential') class\n",
    "        self.modules = []\n",
    "        self.outdims = []\n",
    "        self.m_params_count = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "        self.outdims.append(0)\n",
    "        self.m_params_count.append(len(module.getParameters()))\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = []\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            output = self.modules[i].updateOutput(np.array([input[j][i] for j in range(len(input))]))\n",
    "            self.output.append(output)\n",
    "            self.outdims[i] = output.shape[1]\n",
    "        self.output = np.hstack(self.output)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        self.gradInput = []\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            left = np.sum(self.outdims[:i])\n",
    "            right = np.sum(self.outdims[:i + 1])\n",
    "            gradInput = mod.backward(np.array([input[j][i] for j in range(len(input))]), gradOutput[:,left:right])\n",
    "            self.gradInput.append(gradInput)\n",
    "        self.gradInput = apply(zip, self.gradInput)\n",
    "                \n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def setParameters(self, parameters):\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            left = np.sum(self.m_params_count[:i])\n",
    "            right = np.sum(self.m_params_count[:i + 1])\n",
    "            mod.setParameters(parameters[left:right])\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = []\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            params.append(mod.getParameters())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        params = []\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            params.append(mod.getGradParameters())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def updateParameters(self, delta):\n",
    "        for i, mod in enumerate(self.modules):\n",
    "            left = np.sum(self.m_params_count[:i])\n",
    "            right = np.sum(self.m_params_count[:i + 1])\n",
    "            mod.updateParameters(delta[left:right])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Merge\"\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ebdf430acb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mapplies\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfully\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconnected\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInnerProductLayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out, l2=0):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        self.l2 = l2\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.array([np.dot(self.W, input[i]) + self.b for i in range(len(input))])\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.array([np.dot(self.W.T, gradOutput[i]) for i in range(len(input))])\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW += np.sum([np.dot(gradOutput[i][:,np.newaxis], input[i][np.newaxis,:]) \n",
    "                             for i in range(len(input))], axis=0) + self.W * self.l2\n",
    "        self.gradb += np.sum(gradOutput, axis=0) + self.b * self.l2\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def setParameters(self, parameters):\n",
    "        shape = self.W.shape\n",
    "        self.W = self.W.ravel()\n",
    "        self.W = parameters[:self.W.size]\n",
    "        self.W = self.W.reshape(shape)\n",
    "        self.b = parameters[self.W.size:]\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return np.concatenate([self.W.ravel(), self.b])\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return np.concatenate([self.gradW.ravel(), self.gradb])\n",
    "    \n",
    "    def updateParameters(self, delta):\n",
    "        print delta\n",
    "        shape = self.W.shape\n",
    "        self.W = self.W.ravel()\n",
    "        self.W += delta[:self.W.size]\n",
    "        self.W = self.W.reshape(shape)\n",
    "        self.b += delta[self.W.size:]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1d3f3245e700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# This is a nice initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "class Conv(Module):\n",
    "    def __init__(self, n_features, n_filters):\n",
    "        super(Conv, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_features)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_filters, n_features))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_filters)\n",
    "        self.normalizeWeights()\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        self.argmax = []\n",
    "        \n",
    "    def normalizeWeights(self):\n",
    "        self.W = self.W / np.sum(self.W ** 2, axis=1, keepdims=True) ** 0.5\n",
    "        \n",
    "    def updateOutput(self, input):        \n",
    "        self.output = []\n",
    "        self.argmax = []\n",
    "        for i in range(len(input)):   \n",
    "            if len(input[i]) == 0:\n",
    "                self.output.append(np.zeros((self.W.shape[0])))\n",
    "            else:\n",
    "                products = [np.dot(input[i], filt) + b for filt, b in zip(self.W, self.b)]\n",
    "                self.argmax.append(np.argmax(products, axis=1))\n",
    "                self.output.append(np.max(products, axis=1))\n",
    "        self.output = np.array(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = []\n",
    "        for i in range(len(gradOutput)):\n",
    "            gradInput = np.zeros_like(input[i])\n",
    "            if len(input[i]) != 0:           \n",
    "                for j in range(len(gradOutput[i])): \n",
    "                    gradInput[self.argmax[i][j]] = self.W[j] * gradOutput[i][j]\n",
    "            self.gradInput.append(gradInput)\n",
    "        self.gradInput = np.array(self.gradInput)\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        for i in range(len(gradOutput)):\n",
    "            if len(input[i]) != 0:\n",
    "                for j in range(len(gradOutput[i])):\n",
    "                    self.gradW[j] += input[i][self.argmax[i][j]] * gradOutput[i][j]\n",
    "                    self.gradb[j] += gradOutput[i][j]\n",
    "       \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def setParameters(self, parameters):\n",
    "        shape = self.W.shape\n",
    "        self.W = self.W.ravel()\n",
    "        self.W = parameters[:self.W.size]\n",
    "        self.W = self.W.reshape(shape)\n",
    "        self.b = parameters[self.W.size:]\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return np.concatenate([self.W.ravel(), self.b])\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return np.concatenate([self.gradW.ravel(), self.gradb])\n",
    "    \n",
    "    def updateParameters(self, delta):\n",
    "        print delta\n",
    "        shape = self.W.shape\n",
    "        self.W = self.W.ravel()\n",
    "        self.W += delta[:self.W.size]\n",
    "        self.W = self.W.reshape(shape)\n",
    "        self.b += delta[self.W.size:]\n",
    "        self.normalizeWeights()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Conv: %d features, %d filters' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c59ab28f833b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdateOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "class InputLayer(Module):\n",
    "    def __init__(self):\n",
    "        super(InputLayer, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput\n",
    "        return self.gradInput\n",
    "        \n",
    "    def __repr__(self):\n",
    "        q = 'InputLayer'\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is probably the hardest but as others only takes 5 lines of code in total. \n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        \n",
    "        t = np.exp(input)\n",
    "        self.output = np.divide(t, np.sum(t, axis=1, keepdims=True))\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = []\n",
    "        for k in range(len(input)):\n",
    "            jacob = [[self.output[k,i] * ((i == j) - self.output[k,j]) for i in range(len(self.output[k]))]\\\n",
    "                                                                       for j in range(len(self.output[k]))]\n",
    "            self.gradInput.append(np.dot(jacob, gradOutput[k]))\n",
    "        self.gradInput = np.array(self.gradInput)\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "            \n",
    "        self.slope = slope\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input\n",
    "        self.output[input < 0] *= self.slope\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput\n",
    "        self.gradInput[input < 0] *= self.slope\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input\n",
    "        self.output[self.output <= 0] = self.alpha * (np.exp(self.output[self.output <= 0]) - 1)\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        grad = np.zeros_like(input)\n",
    "        grad[input > 0] = 1\n",
    "        grad[input <= 0] = self.output[input <= 0] + self.alpha\n",
    "        self.gradInput = np.multiply(gradOutput, grad)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterions are used to score the models answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Criterion:\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](https://www.kaggle.com/wiki/MultiClassLogLoss). Nevertheless there is a sum over `y` (target) in that formula, \n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def updateOutput(self, input, target): \n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15) )\n",
    "        \n",
    "        self.output = -np.sum(target * np.log(input_clamp)) / len(target)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.maximum(1e-15, np.minimum(input, 1 - 1e-15) )\n",
    "                \n",
    "        self.gradInput = -target / input_clamp  / len(target)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS LEVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section is **optional** and is for people who want to push their accuracy to the **max**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. \n",
    "\n",
    "This is a very cool regularizer, that can really boost your model. In fact, when you see your net is overfitting try to add more dropout.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch). When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.mask = np.random.choice([0, 1], size=input.shape, p=[1 - self.p, self.p])\n",
    "        self.output = input * self.mask\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.grad = np.zeros_like(input)\n",
    "        self.grad[self.mask == 1] = 1\n",
    "        \n",
    "        return self.gradInput * grad\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another significant recent ideaa that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement a part of the layer: mean subtraction. That is, the module should calculate mean value for every feature (every column) and subtract it.\n",
    "\n",
    "Note, that you need to estimate the mean over the dataset to be able to predict on test examples. The right way is to create a variable which will hold smoothed mean over batches (exponential smoothing works good) and use it when forwarding test examples.\\n\",\n",
    "When training calculate mean as folowing:\n",
    "\n",
    "$meanToSubtract = self.oldMean * alpha + batchMean * (1 - alpha)$\n",
    "\n",
    "when evaluating (`self.training == False`) set $alpha = 1$\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchMeanSubtraction(Module):\n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchMeanSubtraction, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.old_mean = None \n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BatchMeanNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
